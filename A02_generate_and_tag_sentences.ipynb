{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"A02_generate_and_tag_sentences.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3.9.2 64-bit ('virtenv')","metadata":{"interpreter":{"hash":"3d45b6fd2b3f4abcdbfdb01e738047dae6b425b82bea5b3c14c75d8eb227731e"}}},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.2-final"}},"cells":[{"cell_type":"markdown","metadata":{"id":"xM3TUnWwZutX"},"source":["# 02 Generate and tag sentences"]},{"cell_type":"markdown","metadata":{"id":"WNDOagyTW6bz"},"source":["\n","## Script A02\n","\n","This script generates the training dataset, containing questions and the   corresponding tags (such as granularity and datasets) in the Spacy-format so it can be used for the training.\n","\n","It loads a csv-file with pre-written questions that contain brackets to insert elements such as {dataset} or {year} or {granularity} etc.  `(see input/question_generator.csv)` Currently, we only have seven questions, please feel free to add more. For every dataset we have, several questions are being generated randomly.\n","\n","A lot could be improved in this script. At the moment, it only takes data of the so called \"Gemeindeportraet\" of the Statistics Office of the Canton of Zurich. This is because those data variables are in the same format and use the same variable names. We want of course to get away from this structure and use all kind of statistical OGD.\n","\n","## Thoughts for improvement\n","\n","- Include other useful **custom tags**\n","- **Insert important elements** correctly, such as year data, locality level, etc.\n","- Improve **generating the questions**: i.e. by insterting many more ways of saying 'take the three first elements', 'take all elements larger than X', etc. etc.\n","- Optional: Try and **tag every single** word in the datasets as dataset element to improve recognition (and then later use this to attribute them to the right dataset) <br><br>\n","\n","- Outlook for later: Try and **combine this script with the generation of training data** so it can be used for an approach with seq2SQL or similar\n"]},{"cell_type":"code","metadata":{"id":"ES7xxOhwW6cA"},"source":["from __future__ import print_function, unicode_literals\n","import sys\n","import os\n","import pandas as pd\n","import numpy as np\n","from random import sample\n","import io, csv\n","import re\n","import random\n","import json\n","random.seed(12345)"],"execution_count":3,"outputs":[]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["OUT_DIR = \"output\"\n","if not os.path.exists(OUT_DIR):\n","    os.mkdir(OUT_DIR)\n","DATA_DIR = \"data\"\n","INPUT_DIR = \"input\""]},{"cell_type":"code","metadata":{"id":"ui2KdJ0lW6cC"},"source":["def _uniform_cleaning(str_in):\n","    str_in=str_in.replace(\"-\",\"\")\n","    str_in=str_in.replace(\"(\",\"\")\n","    str_in=str_in.replace(\")\",\"\")\n","    #str_in=str_in.replace(\"ü\",\"ue\")\n","    #str_in=str_in.replace(\"ä\",\"ae\")\n","    #str_in=str_in.replace(\"ö\",\"oe\")\n","    return(str_in)"],"execution_count":5,"outputs":[]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["def _add_data_type_column(df):\n","    # create extra column from dataset title\n","    # where data type is specified\n","    df['question_type'] = df['dataset_title'].str.extract(r\"\\[(.*?)\\]\", expand=False)\n","    df['question_type'] = np.where(df['question_type'] == '%', \"percent\", \"cardinal\")\n","    df['dataset_title'] = df['dataset_title'].str.replace(r\"\\[(.*?)\\]\", \"\")\n","    return df"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["def _sample_locality(data, random_value):\n","    if random_value == \"one locality\":\n","                locality_insert = \"in \" + sample(list(data['GEBIET_NAME']), 1)[0]\n","    elif random_value == \"one level\":\n","        locality_insert = sample([\"für den gesamten Kanton\",\"im Kanton Zürich\",\"auf Bezirksebene\",\n","                                \"für alle Bezirke\",\"pro Bezirk\",\"auf Gemeindeebene\",\"für alle Gemeinden\",\"pro Gemeinde\"], 1)[0]\n","    elif random_value == \"several localities\":\n","        locality_insert = \"\"\n","        local_loop = sample([1,2,3],1)[0]\n","        for local in range(0,local_loop):\n","            if local != 0 and local != (local_loop - 1):\n","                locality_insert += \", \"\n","            if local != 0 and local == (local_loop - 1):\n","                locality_insert += \" und \"\n","            locality_insert += sample(list(data['GEBIET_NAME']), 1)[0]\n","    return locality_insert"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["def _sample_time(data, random_value):\n","    years = data['INDIKATOR_JAHR'].tolist()\n","\n","    if random_value in [\"aktuellste\", \"neueste\"]:\n","        exact_value = max(years)\n","        return exact_value, random_value\n","\n","    elif random_value in [\"früheste\", \"älteste\"]:\n","        exact_value = min(years)\n","        return exact_value, random_value\n","\n","    elif random_value == \"year\":\n","        return random.choice(years), \"in \" + str(random.choice(years))\n","\n","    elif random_value == \"span\":\n","        tmp_years = sorted(random.sample(years, 2))\n","        return tmp_years, \"von \" + str(tmp_years[0]) + \" bis \" + str(tmp_years[1])\n"]},{"cell_type":"code","execution_count":76,"metadata":{},"outputs":[],"source":["def _generate_sentences(templates, question_type, dataset_title, retrieval_data, filter_vars): \n","    relevant_templates = templates.loc[templates.main_type == question_type]\n","    #sentences, solution_dicts = list(), list()\n","    ret, tagged = list(), list()\n","    for _, column in relevant_templates.iterrows():\n","        tmp_template = column.question\n","        orig_template = tmp_template\n","        solution_dict = dict()\n","        # replace placeholder with column title extracted above  \n","        tmp_template = tmp_template.replace(\"{main}\", dataset_title)\n","        solution_dict[\"main\"] = dataset_title\n","\n","        tmp_template = tmp_template.replace(\"{localitylevel}\", \"\")#temporary: empty\n","        solution_dict[\"localitylevel\"] = \"\"\n","\n","        #TODO either one locality, one level, or several localities\n","        random_loc = sample([\"one locality\", \"one level\", \"several localities\"], 1)[0]\n","        solution_dict[\"type_locality\"] = random_loc\n","        locality_insert = _sample_locality(retrieval_data, random_loc)\n","        tmp_template = tmp_template.replace(\"{locality}\", locality_insert)\n","        solution_dict[\"locality_insert\"] = locality_insert\n","\n","        random_year = sample([\"year\", \"span\", \"aktuellste\", \"älteste\"], 1)[0]#\"neueste\", \"früheste\",\n","        retrieval_time, time_insert = _sample_time(retrieval_data, random_year)\n","        tmp_template = tmp_template.replace(\"{yeartime}\", time_insert)\n","        solution_dict[\"type_year\"] = random_year\n","        solution_dict[\"yeartime\"] = retrieval_time\n","        solution_dict[\"stringtime\"] = time_insert\n","\n","        filter_insert = sample(filter_vars, 1)[0]\n","        tmp_template = tmp_template.replace(\"{filter}\", filter_insert)\n","        solution_dict[\"filter\"] = filter_insert\n","\n","        sentence = tmp_template\n","        \"\"\"\n","        for mat in re.findall(r'.*?\\[(.*)].*', tmp_template):\n","            which_part = sample([1,2],1)\n","            if which_part==1:\n","                tmp_template = tmp_template.replace(\"[\"+mat+\"]\", mat.partition(\"|\")[0])\n","            else:\n","                tmp_template = tmp_template.replace(\"[\"+mat+\"]\", mat.partition(\"|\")[2])\n","        \"\"\"\n","        #now the symbol - has to be deleted as it gives issues\n","        sentence =  _uniform_cleaning(tmp_template)\n","        solution_dict[\"sentence\"] = sentence\n","\n","        tagged_tuple = _tag_generated_sentence(solution_dict)#['sentence'], dataset_title)\n","\n","        tagged.append(tagged_tuple)\n","        ret.append(solution_dict)\n","    return ret, tagged\n","\n"]},{"cell_type":"code","execution_count":81,"metadata":{},"outputs":[],"source":["def _tag_generated_sentence(generated_solution, tag_types={\n","                    'main': \"GRAN\",\n","                    'locality_insert': \"PLACE\",\n","                    'stringtime': \"TIME\",\n","            }):\n","    #{'main': 'Eigenkapital', 'localitylevel': '', 'type_locality': 'several localities', 'locality_insert': 'Dänikon und Region Weinland', 'type_year': 'year', 'yeartime': 2006, 'filter': '', 'sentence': 'Wie viel Eigenkapital hat  Dänikon und Region Weinland in 1991 ?'}\n","    ret = {\"entities\" : []}\n","    sentence = generated_solution[\"sentence\"]\n","    for key, tag in tag_types.items():\n","        search_for = generated_solution[key]\n","        match = re.search(search_for, sentence, flags=re.IGNORECASE)\n","        if match:\n","            ret[\"entities\"].append([match.start(), match.end(), tag])\n","    \n","        \n","    return [sentence, ret]\n","\n"]},{"cell_type":"code","execution_count":68,"metadata":{},"outputs":[],"source":["def _get_filter_vars(value_types):\n","    filtered_values = [i for i in value_types if i not in [\"INDIKATOR_JAHR\", \"GEBIET_NAME\", \"BFS_NR\", \"INDIKATOR_VALUE\"]]\n","    filtered_values.append(\"\")\n","    return filtered_values"]},{"cell_type":"code","execution_count":77,"metadata":{},"outputs":[],"source":["def generate_training_data(data, templates, outfile_ner, outfile_ir):\n","    \"\"\"\n","    :param pd.Dataframe data: df with string data\n","    :param pd.Dataframe templates: df containing templates for query generation\n","    :param string outfile_ner: file to safe training data for NER\n","    :param string outfile_ir: file to write training data for information retrieval\n","    \"\"\"\n","    tagged_sentences, solutions = list(), list()\n","    data = _add_data_type_column(data)\n","    for id_, group in data.groupby('index'):\n","        fname_retrieval_data = os.path.join(DATA_DIR, str(id_) + \".csv\")\n","        problematic_files = list()\n","        try:\n","            with open(fname_retrieval_data, \"r\", encoding=\"utf-8\") as inf:\n","                retrieval_data = pd.read_csv(inf, sep=\";|:\")\n","        except:#unicodedecode, pd.errors.ParserError:\n","            problematic_files.append(fname_retrieval_data)\n","        \n","        value_types = group['var'].tolist()#todo: rename var column -> reserved vocab in pandas\n","        dataset_title = group.dataset_title.unique()[0]\n","        question_type = group.question_type.unique()[0]\n","        \n","        # temporary: only take columns containing the main value of the gemeindeportraet dataset----------\n","        if \"INDIKATOR_VALUE\" in value_types:\n","            #title of the column \n","            dataset_title = _uniform_cleaning(dataset_title.strip())\n","\n","            # temporary: because it is standardized\n","            filter_vars = _get_filter_vars(value_types)\n","            \n","            generated_solutions, tagged_tuples = _generate_sentences(templates, question_type, dataset_title, retrieval_data, filter_vars)\n","            \n","            tagged_sentences += tagged_tuples\n","            solutions += generated_solutions\n","\n","    return tagged_sentences, solutions\n","\n","\n"]},{"cell_type":"code","execution_count":82,"metadata":{"tags":[]},"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-6-0ca5468b0b90>:6: FutureWarning: The default value of regex will change from True to False in a future version.\n  df['dataset_title'] = df['dataset_title'].str.replace(r\"\\[(.*?)\\]\", \"\")\n<ipython-input-77-6c55996b6cdd>:15: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n  retrieval_data = pd.read_csv(inf, sep=\";|:\")\n"]}],"source":["# load dataset descriptions and template sentences \n","overview_data = pd.read_csv(os.path.join(DATA_DIR, \"datasets_overview.csv\"))\n","template_data = pd.read_csv(os.path.join(INPUT_DIR, \"question_generator.csv\"))\n","\n","\n","tagged_sentences, solutions = generate_training_data(data=overview_data,\n","                        templates=template_data,\n","                        outfile_ner=os.path.join(OUT_DIR, \"spacy_training_sentences.json\"),\n","                        outfile_ir=os.path.join(OUT_DIR, \"info_retrieval_training_data.csv\")\n","                        )\n"]},{"cell_type":"code","execution_count":84,"metadata":{},"outputs":[],"source":["# save data\n","with open(os.path.join(INPUT_DIR, \"tagged_sentences_latest.json\"), \"w\", encoding='utf-8') as outfile:\n","    json.dump(tagged_sentences, outfile, ensure_ascii=False)\n","\n","training_data_ir = pd.DataFrame(solutions)\n","with open(os.path.join(INPUT_DIR, \"info_retrieval_data_latest.csv\"), \"w\") as outf:\n","    training_data_ir.to_csv(outf)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}]}